{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils.loss_function import LossFunction\n",
    "from utils.activation import Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron(object):\n",
    "        \"\"\"This is a single layer neural network classifier.\n",
    "        Parameters:\n",
    "        -----------\n",
    "        train_x: input numpy array\n",
    "            e.g., train_x = np.array([0.3]).\n",
    "        train_x: actual output numpy array\n",
    "            e.g., train_y = np.array([0.3]).\n",
    "        hidden_neurons: integer\n",
    "            number of neurons/Perceptron\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, train_x, train_y, hidden_neurons=1, bias=0):\n",
    "            self.train_x = train_x\n",
    "            self.train_y = train_y\n",
    "\n",
    "            self.hidden_neurons = hidden_neurons\n",
    "            self.input_nodes = np.shape(train_x)[0]\n",
    "            self.output_nodes = np.shape(train_y)[0]\n",
    "            self.bias = bias\n",
    "\n",
    "            # seed for the fixed random values\n",
    "            np.random.seed(1)\n",
    "            self.W_in = np.random.normal(0.0, 0.1, (self.input_nodes, self.hidden_neurons))\n",
    "            print(self.W_in)\n",
    "\n",
    "        def forward_prop(self):\n",
    "            self.X = np.dot(self.train_x.T, self.W_in) + self.bias\n",
    "            self.A = Activation.sigmoid(self.X)\n",
    "            self.E = LossFunction.L2_loss(np.array(self.A), np.array(self.train_y))\n",
    "\n",
    "#       backpropagation\n",
    "        def back_prop(self):\n",
    "            self.loss = self.E\n",
    "            self.dloss = - (self.train_y - self.A)*self.A*(1-self.A)*self.train_x\n",
    "\n",
    "#       optimization\n",
    "        \"\"\"Optimizer function for tuning the weight by the gradient of the error.\n",
    "        Parameters:\n",
    "        -----------\n",
    "        lr: float\n",
    "        learning rate\n",
    "        \"\"\"\n",
    "        def optimization(self, lr=0.01):\n",
    "                if self.A - self.train_y > 0:\n",
    "                    self.W_in = self.W_in + lr * self.dloss\n",
    "                else:\n",
    "                    self.W_in = self.W_in - lr * self.dloss\n",
    "\n",
    "#       train the network\n",
    "        \"\"\"Trains the neural network.\n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_iterations: int\n",
    "        number of times to update the weight\n",
    "        \"\"\"\n",
    "        def train(self, n_iterations=10):\n",
    "            for i in range(n_iterations):\n",
    "                # forward pass\n",
    "                self.forward_prop()\n",
    "                self.back_prop()\n",
    "                self.optimization()\n",
    "            print(self.W_in, \"trained Weight\")\n",
    "            print(self.A, \"Predicted Output\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
