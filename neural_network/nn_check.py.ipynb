{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "logging.basicConfig()\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "class Activation(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def relu_activation(x):\n",
    "        return max(0, x)\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    @staticmethod\n",
    "    def tanh(x):\n",
    "        return (1 - np.exp(x)) / (1 + np.exp(x))\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax(x):\n",
    "        x_new = [np.exp(i) for i in x]\n",
    "        sum_x_new = sum(x_new)\n",
    "        return [sum_x_new / (i) for i in x_new]\n",
    "\n",
    "    @staticmethod\n",
    "    def derivate_relu(x):\n",
    "        if x > 0:\n",
    "            return 1\n",
    "        return 0\n",
    "\n",
    "    @staticmethod\n",
    "    def derivate_sigmoid(x):\n",
    "        return (Activation.sigmoid(x)) * (1 - Activation.sigmoid(x))\n",
    "\n",
    "    @staticmethod\n",
    "    def derivate_tanh(x):\n",
    "        return - np.exp(x) / (1 + np.exp(x)) ** 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossFunction(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def cross_entropy(Y_pred, Y_train):\n",
    "        if Y_pred == 1:\n",
    "            return -np.log(Y_train)\n",
    "        else:\n",
    "            return -np.log(1 - Y_train)\n",
    "\n",
    "    @staticmethod\n",
    "    def hinge_loss(Y_pred, Y_train):\n",
    "        return np.max(0, 1 - Y_pred * Y_train)\n",
    "\n",
    "    @staticmethod\n",
    "    def L1_loss(Y_pred, Y_train):\n",
    "        return np.sum(np.absolute(Y_pred - Y_train))\n",
    "\n",
    "    @staticmethod\n",
    "    def L2_loss(Y_pred, Y_train):\n",
    "        return np.sum(np.power((Y_pred - Y_train), 2)) / len(Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "\n",
    "        def __init__(self, train_x, train_y, hidden_layer=1, hidden_neurons=3, bias = 1):\n",
    "            self.train_x = train_x\n",
    "            self.train_y = train_y\n",
    "            \n",
    "            self.hidden_layer = hidden_layer\n",
    "            self.hidden_neurons = hidden_neurons\n",
    "            self.input_nodes = np.shape(train_x)[0]\n",
    "            self.output_nodes = np.shape(train_y)[0]\n",
    "            self.bias = bias\n",
    "\n",
    "            self.loop_counter = self.hidden_layer + 1\n",
    "            # seed for the fixed random values\n",
    "            np.random.seed(3)\n",
    "            self.W_in = np.random.normal(0.0, 0.1, (self.input_nodes, self.hidden_neurons))\n",
    "            self.W_out = np.random.normal(0.0, 0.1, (self.hidden_neurons, self.output_nodes))\n",
    "            self.bias_weight_Mat = np.random.normal(0.0, 0.1, (1,self.loop_counter))\n",
    "            \n",
    "            # special case to check, override bias_weight_Mat\n",
    "            self.bias_weight_Mat = [0.35, 0.65]          \n",
    "            print(self.W_in, self.W_out, \"First and last weight mat\")\n",
    "            self.Weight_Mat = list()\n",
    "            self.Weight_Mat.append(self.W_in)\n",
    "            \n",
    "            if self.loop_counter > 2:\n",
    "                for i in range(1, self.loop_counter-1):\n",
    "                    self.Wh_i = np.random.normal(0.0, 0.1, (self.hidden_neurons, self.hidden_neurons))\n",
    "                    self.Weight_Mat.append(self.Wh_i)\n",
    "            self.Weight_Mat.append(self.W_out)\n",
    "             \n",
    "                    \n",
    "        def L2_loss(self, Y_pred, Y_train):\n",
    "            return np.sum(np.power((Y_pred - Y_train), 2)) / len(Y_train)\n",
    "        \n",
    "        def sigmoid(self, x):\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "        \n",
    "        def derivative_L2_loss(x):\n",
    "            return -x\n",
    "\n",
    "        def derivative_L1_loss(x):\n",
    "            return -1\n",
    "\n",
    "        def forward_prop(self):\n",
    "            weight = self.Weight_Mat\n",
    "            # Layewise activation output\n",
    "            self.lw_net = list()\n",
    "            self.lw_act_out = list()\n",
    "            \n",
    "            for i in range(len(weight)):\n",
    "                print((weight[i]), i)\n",
    "                print((self.train_x), i)\n",
    "                X_i = np.dot(np.array(self.train_x).T, np.array(weight[i])) + self.bias_weight_Mat[i]\n",
    "                self.lw_net.append(X_i.tolist()[0])\n",
    "                A_i = self.sigmoid(X_i)\n",
    "                self.lw_act_out.append(A_i.tolist()[0])\n",
    "                self.train_x = (A_i).T\n",
    "            self.E_total = 0\n",
    "            \n",
    "            for i in range(len(A_i)):\n",
    "                E = self.L2_loss(np.array(A_i[i]), np.array(self.train_y[i]))\n",
    "                self.E_total += E\n",
    "            print(\"lw\", self.lw_net)\n",
    "            print(\"lw_ac_out\", self.lw_act_out)\n",
    "            print(\"total\", self.E_total)\n",
    "                        \n",
    "        \n",
    "#       backpropagation\n",
    "        def back_prop(self):\n",
    "            self.loss = self.E_total\n",
    "            for i in reversed(range(self.loop_counter)):\n",
    "                # output layer gradient\n",
    "                self.grad_out = list()\n",
    "                recur_grad = list()\n",
    "                for k in range(len(self.lw_act_out[i])):\n",
    "                            dloss_to_out = self.train_y[k][0] - self.lw_act_out[i][k]\n",
    "                            dout_to_net = self.lw_act_out[i][k]*(1-self.lw_act_out[i][k])\n",
    "                            recur_grad.append(dloss_to_out*dout_to_net)\n",
    "                            print(recur_grad, \"recur\")\n",
    "                            for p in range(len(self.lw_net[i])):\n",
    "                                    dnet_to_weight = self.lw_net[i][p]\n",
    "                                    self.grad_out.append(dloss_to_out*dout_to_net*dnet_to_weight)\n",
    "                print(self.grad_out, \"out\")\n",
    "            return self.grad_out\n",
    "                \n",
    "                \n",
    "                \n",
    "#\n",
    "#             for el in self.lw_act_out[len(self.lw_act_out)-1]:\n",
    "#                     for k in range(len(el)):\n",
    "#                         loss_to_out = self.train_y[k][0] - el[k]\n",
    "#                         out_to_net = el[k]*(1-el[k])\n",
    "#                         for elm in self.lw_net[len(self.lw_net)-1]:\n",
    "#                             for p in range(len(elm)):\n",
    "#                                 net_to_weight = elm[p]\n",
    "#                                 self.grad_out.append(loss_to_out*out_to_net*net_to_weight)\n",
    "                                \n",
    "#             for j in reversed(range(self.loop_counter-1)):\n",
    "#                 for el in self.lw_act_out[j]:\n",
    "#                     for k in range(len(el)):\n",
    "#                         print(el[k])\n",
    "# #                         loss_to_out = self.train_y[k][0] - el[k]\n",
    "# #                         out_to_net = el[k]*(1-el[k])\n",
    "# #                         for elm in self.lw_net[j]:\n",
    "# #                             for p in range(len(elm)):\n",
    "# #                                 net_to_weight = elm[p]\n",
    "# #                                 self.grad_out.append(loss_to_out*out_to_net*net_to_weight)\n",
    "           \n",
    "            \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.17886285  0.04365099  0.00964975]\n",
      " [-0.18634927 -0.02773882 -0.0354759 ]] [[-0.00827415 -0.06270007 -0.00438182]\n",
      " [-0.0477218  -0.13138648  0.08846224]\n",
      " [ 0.0881318   0.17095731  0.00500336]] First and last weight mat\n",
      "[[ 0.17886285  0.04365099  0.00964975]\n",
      " [-0.18634927 -0.02773882 -0.0354759 ]] 0\n",
      "[[0.5], [0.1]] 0\n",
      "[[-0.00827415 -0.06270007 -0.00438182]\n",
      " [-0.0477218  -0.13138648  0.08846224]\n",
      " [ 0.0881318   0.17095731  0.00500336]] 1\n",
      "[[0.60367383]\n",
      " [0.59122979]\n",
      " [0.58692728]] 1\n",
      "lw [[0.420796496637871, 0.36905161050045543, 0.35127728361091054], [0.6685175218763445, 0.6348095185571316, 0.7025929335798671]]\n",
      "lw_ac_out [[0.603673828796172, 0.5912297942319985, 0.5869272825594465], [0.6611711281141477, 0.6535792046048169, 0.668762408001935]]\n",
      "total 0.3245009690727652\n",
      "None\n",
      "[0.07366551561317096] recur\n",
      "[0.07366551561317096, -0.14571497384806756] recur\n",
      "[0.07366551561317096, -0.14571497384806756, -0.09719431939319241] recur\n",
      "[0.049246687945460214, 0.04676357050065992, 0.05175687071833129, -0.09741301321718647, -0.0925012523950568, -0.1023783109424274, -0.06497610554119493, -0.061699879100480554, -0.06828804198976163] out\n",
      "[0.09242920759512628] recur\n",
      "[0.09242920759512628, -0.14046994542763822] recur\n",
      "[0.09242920759512628, -0.14046994542763822, -0.08653475229269031] recur\n",
      "[0.038893886743043636, 0.03411114792026228, 0.0324682809703249, -0.05910926091886309, -0.05184065958698097, -0.049343900858793595, -0.03641352060219006, -0.03193578969787533, -0.030397692723319266] out\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.038893886743043636,\n",
       " 0.03411114792026228,\n",
       " 0.0324682809703249,\n",
       " -0.05910926091886309,\n",
       " -0.05184065958698097,\n",
       " -0.049343900858793595,\n",
       " -0.03641352060219006,\n",
       " -0.03193578969787533,\n",
       " -0.030397692723319266]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn = NeuralNetwork([[0.5], [0.1]], [[0.99], [0.01], [0.23]], 1, 3)\n",
    "print(nn.forward_prop())\n",
    "nn.back_prop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
