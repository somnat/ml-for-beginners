{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "logging.basicConfig()\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "class Activation(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def relu_activation(x):\n",
    "        return max(0, x)\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    @staticmethod\n",
    "    def tanh(x):\n",
    "        return (1 - np.exp(x)) / (1 + np.exp(x))\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax(x):\n",
    "        x_new = [np.exp(i) for i in x]\n",
    "        sum_x_new = sum(x_new)\n",
    "        return [sum_x_new / (i) for i in x_new]\n",
    "\n",
    "    @staticmethod\n",
    "    def derivate_relu(x):\n",
    "        if x > 0:\n",
    "            return 1\n",
    "        return 0\n",
    "\n",
    "    @staticmethod\n",
    "    def derivate_sigmoid(x):\n",
    "        return (Activation.sigmoid(x)) * (1 - Activation.sigmoid(x))\n",
    "\n",
    "    @staticmethod\n",
    "    def derivate_tanh(x):\n",
    "        return - np.exp(x) / (1 + np.exp(x)) ** 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossFunction(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def cross_entropy(Y_pred, Y_train):\n",
    "        if Y_pred == 1:\n",
    "            return -np.log(Y_train)\n",
    "        else:\n",
    "            return -np.log(1 - Y_train)\n",
    "\n",
    "    @staticmethod\n",
    "    def hinge_loss(Y_pred, Y_train):\n",
    "        return np.max(0, 1 - Y_pred * Y_train)\n",
    "\n",
    "    @staticmethod\n",
    "    def L1_loss(Y_pred, Y_train):\n",
    "        return np.sum(np.absolute(Y_pred - Y_train))\n",
    "\n",
    "    @staticmethod\n",
    "    def L2_loss(Y_pred, Y_train):\n",
    "        return np.sum(np.power((Y_pred - Y_train), 2)) / len(Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "\n",
    "        def __init__(self, train_x, train_y, hidden_layer=1, hidden_neurons=3, bias = 1):\n",
    "            self.train_x = train_x\n",
    "            self.train_y = train_y\n",
    "            \n",
    "            self.hidden_layer = hidden_layer\n",
    "            self.hidden_neurons = hidden_neurons\n",
    "            self.input_nodes = np.shape(train_x)[0]\n",
    "            self.output_nodes = np.shape(train_y)[0]\n",
    "            self.grad_out = list()\n",
    "            self.bias = bias\n",
    "\n",
    "            self.loop_counter = self.hidden_layer + 1\n",
    "            # seed for the fixed random values\n",
    "            np.random.seed(3)\n",
    "            self.W_in = np.random.normal(0.0, 0.1, (self.input_nodes, self.hidden_neurons))\n",
    "            self.W_out = np.random.normal(0.0, 0.1, (self.hidden_neurons, self.output_nodes))\n",
    "            self.bias_weight_Mat = np.random.normal(0.0, 0.1, (1,self.loop_counter))\n",
    "            \n",
    "            # special case to check, override bias_weight_Mat\n",
    "            self.bias_weight_Mat = [0.35, 0.65]          \n",
    "            print(self.W_in, self.W_out, \"First and last weight mat\")\n",
    "            self.Weight_Mat = list()\n",
    "            self.Weight_Mat.append(self.W_in)\n",
    "            \n",
    "            if self.loop_counter > 2:\n",
    "                for i in range(1, self.loop_counter-1):\n",
    "                    self.Wh_i = np.random.normal(0.0, 0.1, (self.hidden_neurons, self.hidden_neurons))\n",
    "                    self.Weight_Mat.append(self.Wh_i)\n",
    "            self.Weight_Mat.append(self.W_out)\n",
    "             \n",
    "                    \n",
    "        def L2_loss(self, Y_pred, Y_train):\n",
    "            return np.sum(np.power((Y_pred - Y_train), 2)) / len(Y_train)\n",
    "        \n",
    "        def sigmoid(self, x):\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "\n",
    "        def forward_prop(self):\n",
    "            weight = self.Weight_Mat\n",
    "            # Layewise activation output\n",
    "            self.lw_net = list()\n",
    "            self.lw_act_out = list()\n",
    "            \n",
    "            for i in range(len(weight)):\n",
    "                print((weight[i]), i)\n",
    "                print((self.train_x), i)\n",
    "                X_i = np.dot(np.array(self.train_x).T, np.array(weight[i])) + self.bias_weight_Mat[i]\n",
    "                self.lw_net.append(X_i)\n",
    "                A_i = self.sigmoid(X_i)\n",
    "                self.lw_act_out.append(A_i)\n",
    "                self.train_x = (A_i).T\n",
    "            self.E_total = 0\n",
    "            \n",
    "            for i in range(len(A_i)):\n",
    "                E = self.L2_loss(np.array(A_i[i]), np.array(self.train_y[i]))\n",
    "                self.E_total += E\n",
    "            print(\"lw\", self.lw_net)\n",
    "            print(\"lw_ac_out\", self.lw_act_out)\n",
    "            print(\"total\", self.E_total)\n",
    "                        \n",
    "        \n",
    "#       backpropagation\n",
    "        def back_prop(self):\n",
    "            self.loss = self.E_total\n",
    "            for j in reversed(range(self.loop_counter)):\n",
    "                for i in range(len(self.lw_act_out[j])):\n",
    "            \n",
    "                    loss_to_out = self.train_y[i] - self.lw_act_out[j][i]\n",
    "                    out_to_net = self.lw_act_out[j][i]*(1-self.lw_act_out[j][i])\n",
    "                    for k in range(len(self.lw_net[j])):\n",
    "                        net_to_weight_k = self.lw_net[j][k]\n",
    "                        self.grad_out.append(loss_to_out*out_to_net*net_to_weight_k)\n",
    "\n",
    "            return self.grad_out\n",
    "#             for i in reversed(range(self.loop_counter)):\n",
    "#                 for j in range()\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.17886285  0.04365099  0.00964975]\n",
      " [-0.18634927 -0.02773882 -0.0354759 ]] [[-0.00827415 -0.06270007]\n",
      " [-0.00438182 -0.0477218 ]\n",
      " [-0.13138648  0.08846224]] First and last weight mat\n",
      "[[ 0.17886285  0.04365099  0.00964975]\n",
      " [-0.18634927 -0.02773882 -0.0354759 ]] 0\n",
      "[[0.5], [0.1]] 0\n",
      "[[-0.00827415 -0.06270007]\n",
      " [-0.00438182 -0.0477218 ]\n",
      " [-0.13138648  0.08846224]] 1\n",
      "[[0.60367383]\n",
      " [0.59122979]\n",
      " [0.58692728]] 1\n",
      "lw [array([[0.4207965 , 0.36905161, 0.35127728]]), array([[0.56530015, 0.63585596]])]\n",
      "lw_ac_out [array([[0.60367383, 0.59122979, 0.58692728]]), array([[0.637678  , 0.65381609]])]\n",
      "total 0.23715040802471377\n",
      "None\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-7464d866de40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeuralNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.99\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_prop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mback_prop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-0cf7571aafe7>\u001b[0m in \u001b[0;36mback_prop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlw_act_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m                     \u001b[0mloss_to_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlw_act_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m                     \u001b[0mout_to_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlw_act_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlw_act_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlw_net\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork([[0.5], [0.1]], [[0.99], [0.01]], 1, 3)\n",
    "print(nn.forward_prop())\n",
    "print(nn.back_prop())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
